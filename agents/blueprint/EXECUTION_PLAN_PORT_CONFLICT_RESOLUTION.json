{
  "execution_plan": {
    "plan_id": "PORT_CONFLICT_RESOLUTION_002",
    "title": "Port Conflict Resolution and Data Persistence Implementation",
    "version": "1.0.0",
    "created_by": "Claude Sonnet 4",
    "creation_date": "2025-01-28T10:30:00Z",
    "priority": "CRITICAL",
    "status": "IN_PROGRESS"
  },
  "port_conflict_analysis": {
    "current_conflicts": {
      "3000": ["OpenWebUI (AI chat)", "Gitea (moved from 3000)", "Web Dashboard", "Grafana (some configs)"],
      "3001": ["Grafana (monitoring)", "Gitea (moved to 3001)"],
      "8080": ["NextCloud (file storage)", "Web Dashboard (some configs)"],
      "9000": ["Admin Panel", "Web Dashboard (some configs)"]
    },
    "resolved_port_mapping": {
      "3000": "OpenWebUI (AI chat interface - default)",
      "3001": "Grafana (monitoring - unchanged)",
      "3002": "Gitea (Git server - moved from 3001)",
      "8080": "NextCloud (file storage - unchanged)",
      "9000": "Admin Panel (management interface - unchanged)",
      "5000": "Docker Registry (container registry - unchanged)",
      "11434": "Ollama (LLM API - unchanged)",
      "8000": "AI-Q API (knowledge library - unchanged)",
      "9090": "Prometheus (metrics - unchanged)"
    }
  },
  "data_persistence_requirements": {
    "docker_volumes": {
      "ollama": "/opt/ai-q/data/ollama",
      "openwebui": "/opt/ai-q/data/openwebui",
      "gitea": "/opt/ai-q/data/gitea",
      "nextcloud": "/opt/ai-q/data/nextcloud",
      "admin_panel": "/opt/ai-q/data/admin-panel",
      "registry": "/opt/ai-q/data/registry",
      "grafana": "/opt/ai-q/data/grafana",
      "prometheus": "/opt/ai-q/data/prometheus"
    },
    "local_install_paths": {
      "ollama": "/opt/ai-q/local/ollama",
      "openwebui": "/opt/ai-q/local/openwebui",
      "gitea": "/opt/ai-q/local/gitea",
      "nextcloud": "/opt/ai-q/local/nextcloud",
      "admin_panel": "/opt/ai-q/local/admin-panel",
      "registry": "/opt/ai-q/local/registry",
      "grafana": "/opt/ai-q/local/grafana",
      "prometheus": "/opt/ai-q/local/prometheus"
    }
  },
  "implementation_phases": [
    {
      "phase_id": "PHASE_1",
      "title": "Port Conflict Resolution",
      "tasks": [
        {
          "task_id": "TASK_1_1",
          "title": "Move Gitea to Port 3002",
          "description": "Update Gitea configuration from port 3001 to 3002 to avoid conflict with Grafana",
          "files_to_modify": [
            "docker/compose/docker-compose-self-hosted.yml",
            "scripts/gitea-setup.sh",
            "scripts/admin-panel-setup.sh"
          ],
          "estimated_time": "15 minutes"
        },
        {
          "task_id": "TASK_1_2",
          "title": "Update All Port References",
          "description": "Update all configuration files to reflect new port assignments",
          "files_to_modify": [
            "agents/EXECUTION_PLAN_OPENWEBUI_OLLAMA_INTEGRATION.json",
            "recipes/01-infrastructure/07-persistent-data-volumes/05-self-hosted-services.json"
          ],
          "estimated_time": "20 minutes"
        }
      ]
    },
    {
      "phase_id": "PHASE_2",
      "title": "Ollama Service Implementation with Data Persistence",
      "tasks": [
        {
          "task_id": "TASK_2_1",
          "title": "Create Ollama Docker Compose with Volume",
          "description": "Create comprehensive Ollama Docker Compose with persistent data volume",
          "files_to_create": [
            "docker/compose/ollama.yml"
          ],
          "estimated_time": "25 minutes"
        },
        {
          "task_id": "TASK_2_2",
          "title": "Create Ollama Setup Script",
          "description": "Create script to setup Ollama with model pulling and data persistence",
          "files_to_create": [
            "scripts/ollama-setup.sh"
          ],
          "estimated_time": "30 minutes"
        },
        {
          "task_id": "TASK_2_3",
          "title": "Create Model Management Script",
          "description": "Create script to pull and manage models including Gemma3 and CodeLlama",
          "files_to_create": [
            "scripts/ollama-model-manager.sh"
          ],
          "estimated_time": "25 minutes"
        }
      ]
    },
    {
      "phase_id": "PHASE_3",
      "title": "OpenWebUI Service Implementation with Data Persistence",
      "tasks": [
        {
          "task_id": "TASK_3_1",
          "title": "Create OpenWebUI Docker Compose with Volume",
          "description": "Create OpenWebUI Docker Compose with Ollama integration and persistent data",
          "files_to_create": [
            "docker/compose/openwebui.yml"
          ],
          "estimated_time": "25 minutes"
        },
        {
          "task_id": "TASK_3_2",
          "title": "Create OpenWebUI Setup Script",
          "description": "Create script to setup OpenWebUI with Ollama backend and data persistence",
          "files_to_create": [
            "scripts/openwebui-setup.sh"
          ],
          "estimated_time": "30 minutes"
        },
        {
          "task_id": "TASK_3_3",
          "title": "Create OpenWebUI Configuration",
          "description": "Create OpenWebUI configuration for Ollama integration with persistent settings",
          "files_to_create": [
            "config/openwebui/config.json"
          ],
          "estimated_time": "20 minutes"
        }
      ]
    },
    {
      "phase_id": "PHASE_4",
      "title": "Local Installation Support",
      "tasks": [
        {
          "task_id": "TASK_4_1",
          "title": "Create Local Installation Scripts",
          "description": "Create scripts for local installation without Docker",
          "files_to_create": [
            "scripts/local-install.sh",
            "scripts/local-ollama-setup.sh",
            "scripts/local-openwebui-setup.sh"
          ],
          "estimated_time": "40 minutes"
        },
        {
          "task_id": "TASK_4_2",
          "title": "Create Installation Choice Script",
          "description": "Create script to choose between Docker and local installation",
          "files_to_create": [
            "scripts/install-choice.sh"
          ],
          "estimated_time": "20 minutes"
        }
      ]
    },
    {
      "phase_id": "PHASE_5",
      "title": "Recipe Updates and Documentation",
      "tasks": [
        {
          "task_id": "TASK_5_1",
          "title": "Update AI Services Recipes",
          "description": "Update existing recipes with actual implementation details and data persistence",
          "files_to_modify": [
            "recipes/02-ai-services/modules/02-ollama-setup.json",
            "recipes/02-ai-services/modules/03-open-webui-setup.json",
            "recipes/02-ai-services/modules/04-model-management.json"
          ],
          "estimated_time": "35 minutes"
        },
        {
          "task_id": "TASK_5_2",
          "title": "Create Integration Documentation",
          "description": "Create comprehensive documentation for the integration with data persistence",
          "files_to_create": [
            "docs/openwebui-ollama-integration.json",
            "docs/data-persistence-guide.json"
          ],
          "estimated_time": "25 minutes"
        }
      ]
    },
    {
      "phase_id": "PHASE_6",
      "title": "Testing and Verification",
      "tasks": [
        {
          "task_id": "TASK_6_1",
          "title": "Create Integration Test Script",
          "description": "Create test script to verify OpenWebUI-Ollama integration and data persistence",
          "files_to_create": [
            "scripts/test-openwebui-ollama.sh"
          ],
          "estimated_time": "30 minutes"
        },
        {
          "task_id": "TASK_6_2",
          "title": "Update Admin Panel",
          "description": "Update admin panel to include OpenWebUI and Ollama services with correct ports",
          "files_to_modify": [
            "scripts/admin-panel-setup.sh"
          ],
          "estimated_time": "20 minutes"
        }
      ]
    }
  ],
  "final_port_mapping": {
    "3000": "OpenWebUI (AI chat interface)",
    "3001": "Grafana (monitoring)",
    "3002": "Gitea (Git server)",
    "8080": "NextCloud (file storage)",
    "9000": "Admin Panel (management interface)",
    "5000": "Docker Registry (container registry)",
    "11434": "Ollama (LLM API)",
    "8000": "AI-Q API (knowledge library)",
    "9090": "Prometheus (metrics)"
  },
  "data_persistence_configuration": {
    "docker_volumes": {
      "ollama_models": "/opt/ai-q/data/ollama/models",
      "ollama_config": "/opt/ai-q/data/ollama/config",
      "openwebui_data": "/opt/ai-q/data/openwebui/data",
      "openwebui_config": "/opt/ai-q/data/openwebui/config",
      "gitea_data": "/opt/ai-q/data/gitea/data",
      "gitea_config": "/opt/ai-q/data/gitea/config",
      "nextcloud_data": "/opt/ai-q/data/nextcloud/data",
      "nextcloud_config": "/opt/ai-q/data/nextcloud/config"
    },
    "local_paths": {
      "ollama_models": "/opt/ai-q/local/ollama/models",
      "ollama_config": "/opt/ai-q/local/ollama/config",
      "openwebui_data": "/opt/ai-q/local/openwebui/data",
      "openwebui_config": "/opt/ai-q/local/openwebui/config",
      "gitea_data": "/opt/ai-q/local/gitea/data",
      "gitea_config": "/opt/ai-q/local/gitea/config",
      "nextcloud_data": "/opt/ai-q/local/nextcloud/data",
      "nextcloud_config": "/opt/ai-q/local/nextcloud/config"
    }
  },
  "quality_standards": {
    "documentation_format": "JSON only (no markdown)",
    "directory_organization": "No root directory clutter",
    "task_granularity": "Atomic, modular tasks",
    "port_conflicts": "Zero tolerance for port conflicts",
    "data_persistence": "All services must persist data and settings",
    "installation_modes": "Support both Docker and local installation",
    "reproducibility": "All agents must achieve same results"
  }
} 