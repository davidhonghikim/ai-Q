{
  "metadata": {
    "recipe_id": "01-COMPLETE-CORE-INFRASTRUCTURE",
    "title": "Complete Core Infrastructure - Universal Digital Twin Foundation",
    "version": "1.0.0",
    "created_by": "Claude Sonnet 4",
    "estimated_tokens": 87500,
    "total_tasks": 45,
    "estimated_duration": "6-8 hours"
  },
  "overview": {
    "description": "Complete foundational infrastructure setup for Universal Digital Twin system including Docker, Python FastAPI, Node.js React, PostgreSQL, Redis, Minio, and Weaviate",
    "deliverables": [
      "Docker Compose environment with all services",
      "Python FastAPI backend with ML dependencies", 
      "React TypeScript frontend with Material-UI",
      "PostgreSQL database with schemas",
      "Redis cache and session storage",
      "Minio S3-compatible object storage",
      "Weaviate vector database"
    ]
  },
  "tasks": [
    {
      "task_id": "01-001",
      "title": "System Requirements Verification",
      "description": "Verify system meets requirements: 16GB+ RAM, 4+ CPU cores, 100GB+ disk space, Docker Desktop/Engine, Git, Python 3.10+, Node.js 18+",
      "estimated_time": "20 minutes",
      "commands": [
        "systeminfo | findstr Memory",
        "docker --version",
        "python --version",
        "node --version"
      ],
      "acceptance_criteria": [
        "System has 16GB+ RAM",
        "Docker 20.0+ installed",
        "Python 3.10+ available",
        "Node.js 18+ available"
      ]
    },
    {
      "task_id": "01-002", 
      "title": "Project Directory Structure Creation",
      "description": "Create complete directory structure: src/{api,services,models}, frontend/src, docker/, config/, data/, logs/, tests/, docs/, scripts/",
      "estimated_time": "15 minutes",
      "commands": [
        "mkdir -p src/{api,services,models,utils,config}",
        "mkdir -p frontend/{src,public}",
        "mkdir -p docker config data logs tests docs scripts"
      ],
      "acceptance_criteria": [
        "All required directories created",
        "Directory structure follows conventions"
      ]
    },
    {
      "task_id": "01-003",
      "title": "Git Repository Initialization",
      "description": "Initialize Git repository with proper configuration, .gitignore for Python/Node.js/Docker, and initial commit",
      "estimated_time": "10 minutes",
      "commands": [
        "git init",
        "git config user.name 'AI-Q Development'",
        "cat > .gitignore << 'EOF'\n__pycache__/\nnode_modules/\n.env*\nlogs/*.log\nEOF",
        "git add .",
        "git commit -m 'Initial project structure'"
      ],
      "acceptance_criteria": [
        "Git repository initialized",
        "Proper .gitignore configured",
        "Initial commit created"
      ]
    },
    {
      "task_id": "01-004",
      "title": "Docker Desktop Configuration and Resource Allocation",
      "description": "Configure Docker Desktop with optimal settings: 8GB+ RAM, 4+ CPU cores, 64GB disk image, enable BuildKit and experimental features for development",
      "estimated_time": "25 minutes",
      "estimated_tokens": 1900,
      "commands": [
        "docker system info | grep -E 'Total Memory|CPUs'",
        "export DOCKER_BUILDKIT=1",
        "export COMPOSE_DOCKER_CLI_BUILD=1",
        "docker buildx version",
        "docker network create ai-q-network"
      ],
      "acceptance_criteria": [
        "Docker allocated 8GB+ RAM",
        "4+ CPU cores assigned",
        "BuildKit enabled",
        "Custom network created"
      ]
    },
    {
      "task_id": "01-005",
      "title": "Python Virtual Environment and Dependencies Setup",
      "description": "Create Python virtual environment, install FastAPI, SQLAlchemy, Pydantic, transformers, torch, numpy, pandas, redis, minio, weaviate-client, and other ML dependencies",
      "estimated_time": "30 minutes",
      "estimated_tokens": 1850,
      "commands": [
        "python -m venv venv",
        "source venv/bin/activate || venv\\Scripts\\activate",
        "pip install --upgrade pip setuptools wheel",
        "pip install fastapi[all] uvicorn sqlalchemy psycopg2-binary redis minio weaviate-client",
        "pip install torch torchvision transformers numpy pandas scikit-learn",
        "pip freeze > requirements.txt"
      ],
      "acceptance_criteria": [
        "Virtual environment created",
        "All dependencies installed",
        "requirements.txt generated",
        "No installation errors"
      ]
    },
    {
      "task_id": "01-006",
      "title": "Node.js Frontend Project Initialization",
      "description": "Initialize React TypeScript project with Create React App, install Material-UI, React Router, Axios, TypeScript definitions, and development tools",
      "estimated_time": "25 minutes",
      "estimated_tokens": 1750,
      "commands": [
        "cd frontend",
        "npx create-react-app . --template typescript",
        "npm install @mui/material @emotion/react @emotion/styled @mui/icons-material",
        "npm install react-router-dom axios @types/node",
        "npm install --save-dev prettier eslint-config-prettier"
      ],
      "acceptance_criteria": [
        "React TypeScript project created",
        "Material-UI installed",
        "Development tools configured",
        "Package.json updated"
      ]
    },
    {
      "task_id": "01-007",
      "title": "Docker Compose Infrastructure Definition",
      "description": "Create comprehensive docker-compose.yml with PostgreSQL, Redis, Minio, Weaviate, FastAPI backend, React frontend services with proper networking and volumes",
      "estimated_time": "35 minutes",
      "estimated_tokens": 2100,
      "commands": [
        "cat > docker-compose.yml << 'EOF'\nversion: '3.8'\nservices:\n  postgres:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: ai_q_db\n      POSTGRES_USER: ai_q_user\n      POSTGRES_PASSWORD: ai_q_password\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  minio:\n    image: minio/minio:latest\n    command: server /data --console-address \":9001\"\n    environment:\n      MINIO_ROOT_USER: ai_q_minio\n      MINIO_ROOT_PASSWORD: ai_q_minio_password\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    volumes:\n      - minio_data:/data\n\n  weaviate:\n    image: semitechnologies/weaviate:latest\n    ports:\n      - \"8080:8080\"\n    environment:\n      QUERY_DEFAULTS_LIMIT: 25\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n      DEFAULT_VECTORIZER_MODULE: 'none'\n      ENABLE_MODULES: 'text2vec-transformers'\n      TRANSFORMERS_INFERENCE_API: 'http://t2v-transformers:8080'\n    volumes:\n      - weaviate_data:/var/lib/weaviate\n\n  t2v-transformers:\n    image: semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L6-v2\n    environment:\n      ENABLE_CUDA: '0'\n\n  backend:\n    build:\n      context: .\n      dockerfile: docker/python/Dockerfile\n    ports:\n      - \"8000:8000\"\n    environment:\n      DATABASE_URL: postgresql://ai_q_user:ai_q_password@postgres:5432/ai_q_db\n      REDIS_URL: redis://redis:6379\n      MINIO_ENDPOINT: minio:9000\n      WEAVIATE_URL: http://weaviate:8080\n    depends_on:\n      - postgres\n      - redis\n      - minio\n      - weaviate\n    volumes:\n      - ./src:/app/src\n      - ./data:/app/data\n\n  frontend:\n    build:\n      context: ./frontend\n      dockerfile: ../docker/nodejs/Dockerfile\n    ports:\n      - \"3000:3000\"\n    environment:\n      REACT_APP_API_URL: http://localhost:8000\n    depends_on:\n      - backend\n    volumes:\n      - ./frontend/src:/app/src\n\nvolumes:\n  postgres_data:\n  redis_data:\n  minio_data:\n  weaviate_data:\n\nnetworks:\n  default:\n    external:\n      name: ai-q-network\nEOF"
      ],
      "acceptance_criteria": [
        "docker-compose.yml created",
        "All services defined",
        "Proper networking configured",
        "Volumes for data persistence"
      ]
    },
    {
      "task_id": "01-008",
      "title": "PostgreSQL Database Schema Creation",
      "description": "Create comprehensive database schema with tables for users, files, metadata, relationships, processing_jobs, embeddings, and indexes for optimal performance",
      "estimated_time": "40 minutes",
      "estimated_tokens": 2200,
      "commands": [
        "mkdir -p config/database",
        "cat > config/database/schema.sql << 'EOF'\n-- Users table\nCREATE TABLE users (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    email VARCHAR(255) UNIQUE NOT NULL,\n    username VARCHAR(100) UNIQUE NOT NULL,\n    password_hash VARCHAR(255) NOT NULL,\n    is_active BOOLEAN DEFAULT true,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Files table\nCREATE TABLE files (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    user_id UUID REFERENCES users(id) ON DELETE CASCADE,\n    filename VARCHAR(500) NOT NULL,\n    file_path VARCHAR(1000) NOT NULL,\n    file_size BIGINT NOT NULL,\n    mime_type VARCHAR(100),\n    file_hash VARCHAR(64) UNIQUE,\n    storage_backend VARCHAR(50) DEFAULT 'minio',\n    storage_bucket VARCHAR(100),\n    storage_key VARCHAR(500),\n    upload_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- File metadata table\nCREATE TABLE file_metadata (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    file_id UUID REFERENCES files(id) ON DELETE CASCADE,\n    metadata_type VARCHAR(100) NOT NULL,\n    metadata_key VARCHAR(200) NOT NULL,\n    metadata_value TEXT,\n    extracted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Processing jobs table\nCREATE TABLE processing_jobs (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    file_id UUID REFERENCES files(id) ON DELETE CASCADE,\n    job_type VARCHAR(100) NOT NULL,\n    status VARCHAR(50) DEFAULT 'pending',\n    started_at TIMESTAMP WITH TIME ZONE,\n    completed_at TIMESTAMP WITH TIME ZONE,\n    error_message TEXT,\n    result_data JSONB,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Embeddings table\nCREATE TABLE embeddings (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    file_id UUID REFERENCES files(id) ON DELETE CASCADE,\n    embedding_type VARCHAR(100) NOT NULL,\n    chunk_index INTEGER DEFAULT 0,\n    text_content TEXT,\n    vector_id VARCHAR(255),\n    weaviate_class VARCHAR(100),\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Relationships table\nCREATE TABLE relationships (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    source_file_id UUID REFERENCES files(id) ON DELETE CASCADE,\n    target_file_id UUID REFERENCES files(id) ON DELETE CASCADE,\n    relationship_type VARCHAR(100) NOT NULL,\n    confidence_score FLOAT DEFAULT 0.0,\n    metadata JSONB,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Indexes for performance\nCREATE INDEX idx_files_user_id ON files(user_id);\nCREATE INDEX idx_files_file_hash ON files(file_hash);\nCREATE INDEX idx_files_mime_type ON files(mime_type);\nCREATE INDEX idx_file_metadata_file_id ON file_metadata(file_id);\nCREATE INDEX idx_file_metadata_type_key ON file_metadata(metadata_type, metadata_key);\nCREATE INDEX idx_processing_jobs_file_id ON processing_jobs(file_id);\nCREATE INDEX idx_processing_jobs_status ON processing_jobs(status);\nCREATE INDEX idx_embeddings_file_id ON embeddings(file_id);\nCREATE INDEX idx_embeddings_vector_id ON embeddings(vector_id);\nCREATE INDEX idx_relationships_source ON relationships(source_file_id);\nCREATE INDEX idx_relationships_target ON relationships(target_file_id);\nEOF"
      ],
      "acceptance_criteria": [
        "Database schema created",
        "All tables defined with relationships",
        "Indexes created for performance",
        "UUID primary keys configured"
      ]
    },
    {
      "task_id": "01-009",
      "title": "FastAPI Backend Application Structure Creation",
      "description": "Create FastAPI application structure with main.py, routers, dependencies, security, middleware, and configuration management for the Universal Digital Twin backend",
      "estimated_time": "35 minutes",
      "estimated_tokens": 2000,
      "commands": [
        "mkdir -p src/api/{routes,dependencies,security,middleware}",
        "cat > src/main.py << 'EOF'\nfrom fastapi import FastAPI, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.middleware.gzip import GZipMiddleware\nfrom src.api.routes import files, users, search, processing\nfrom src.config.database import init_db\nfrom src.config.settings import get_settings\n\nsettings = get_settings()\n\napp = FastAPI(\n    title=\"AI-Q Universal Digital Twin\",\n    description=\"Multi-modal AI system for comprehensive data ingestion and processing\",\n    version=\"1.0.0\"\n)\n\n# Middleware\napp.add_middleware(GZipMiddleware, minimum_size=1000)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include routers\napp.include_router(users.router, prefix=\"/api/v1/users\", tags=[\"users\"])\napp.include_router(files.router, prefix=\"/api/v1/files\", tags=[\"files\"])\napp.include_router(search.router, prefix=\"/api/v1/search\", tags=[\"search\"])\napp.include_router(processing.router, prefix=\"/api/v1/processing\", tags=[\"processing\"])\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    await init_db()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"AI-Q Universal Digital Twin API\", \"version\": \"1.0.0\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"service\": \"ai-q-backend\"}\nEOF"
      ],
      "acceptance_criteria": [
        "FastAPI main application created",
        "Middleware configured",
        "Router structure defined",
        "Health endpoints available"
      ]
    },
    {
      "task_id": "01-010",
      "title": "Database Models and SQLAlchemy Configuration",
      "description": "Create Pydantic models, SQLAlchemy ORM models, database connection configuration, and session management for PostgreSQL integration",
      "estimated_time": "45 minutes",
      "estimated_tokens": 2300,
      "commands": [
        "mkdir -p src/{models,config}",
        "cat > src/models/database.py << 'EOF'\nfrom sqlalchemy import Column, String, DateTime, Text, BigInteger, Boolean, Float, Integer, ForeignKey\nfrom sqlalchemy.dialects.postgresql import UUID, JSONB\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.sql import func\nimport uuid\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n    \n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    email = Column(String(255), unique=True, nullable=False, index=True)\n    username = Column(String(100), unique=True, nullable=False, index=True)\n    password_hash = Column(String(255), nullable=False)\n    is_active = Column(Boolean, default=True)\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    files = relationship(\"File\", back_populates=\"owner\")\n\nclass File(Base):\n    __tablename__ = \"files\"\n    \n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    user_id = Column(UUID(as_uuid=True), ForeignKey(\"users.id\", ondelete=\"CASCADE\"))\n    filename = Column(String(500), nullable=False)\n    file_path = Column(String(1000), nullable=False)\n    file_size = Column(BigInteger, nullable=False)\n    mime_type = Column(String(100))\n    file_hash = Column(String(64), unique=True, index=True)\n    storage_backend = Column(String(50), default=\"minio\")\n    storage_bucket = Column(String(100))\n    storage_key = Column(String(500))\n    upload_status = Column(String(50), default=\"pending\")\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    owner = relationship(\"User\", back_populates=\"files\")\n    metadata = relationship(\"FileMetadata\", back_populates=\"file\")\n    processing_jobs = relationship(\"ProcessingJob\", back_populates=\"file\")\n    embeddings = relationship(\"Embedding\", back_populates=\"file\")\nEOF"
      ],
      "acceptance_criteria": [
        "SQLAlchemy models created",
        "Relationships defined",
        "UUID primary keys configured",
        "Database configuration ready"
      ]
    },
    {
      "task_id": "01-011", 
      "title": "Pydantic Schemas and API Models",
      "description": "Create comprehensive Pydantic schemas for request/response validation, including user registration, file upload, search queries, and processing job models",
      "estimated_time": "30 minutes",
      "estimated_tokens": 1800,
      "commands": [
        "mkdir -p src/schemas",
        "cat > src/schemas/user.py << 'EOF'\nfrom pydantic import BaseModel, EmailStr, Field\nfrom typing import Optional\nfrom datetime import datetime\nfrom uuid import UUID\n\nclass UserBase(BaseModel):\n    email: EmailStr\n    username: str = Field(..., min_length=3, max_length=100)\n\nclass UserCreate(UserBase):\n    password: str = Field(..., min_length=8)\n\nclass UserUpdate(BaseModel):\n    email: Optional[EmailStr] = None\n    username: Optional[str] = Field(None, min_length=3, max_length=100)\n    password: Optional[str] = Field(None, min_length=8)\n\nclass User(UserBase):\n    id: UUID\n    is_active: bool\n    created_at: datetime\n    updated_at: datetime\n    \n    class Config:\n        from_attributes = True\nEOF",
        "cat > src/schemas/file.py << 'EOF'\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Dict, Any\nfrom datetime import datetime\nfrom uuid import UUID\n\nclass FileBase(BaseModel):\n    filename: str = Field(..., max_length=500)\n    mime_type: Optional[str] = Field(None, max_length=100)\n\nclass FileCreate(FileBase):\n    file_size: int = Field(..., gt=0)\n    file_hash: str = Field(..., max_length=64)\n\nclass FileUpdate(BaseModel):\n    filename: Optional[str] = Field(None, max_length=500)\n    upload_status: Optional[str] = Field(None, max_length=50)\n\nclass File(FileBase):\n    id: UUID\n    user_id: UUID\n    file_path: str\n    file_size: int\n    file_hash: str\n    storage_backend: str\n    storage_bucket: Optional[str]\n    storage_key: Optional[str]\n    upload_status: str\n    created_at: datetime\n    updated_at: datetime\n    \n    class Config:\n        from_attributes = True\nEOF"
      ],
      "acceptance_criteria": [
        "Pydantic schemas created",
        "Validation rules defined",
        "Request/response models ready",
        "Type safety ensured"
      ]
    },
    {
      "task_id": "01-012",
      "title": "Redis Cache Configuration and Session Management",
      "description": "Set up Redis connection, session management, caching utilities, and background task queue for improved performance and scalability",
      "estimated_time": "25 minutes",
      "estimated_tokens": 1600,
      "commands": [
        "mkdir -p src/services/cache",
        "cat > src/services/cache/redis_client.py << 'EOF'\nimport redis.asyncio as redis\nfrom typing import Optional, Any\nimport json\nimport pickle\nfrom src.config.settings import get_settings\n\nsettings = get_settings()\n\nclass RedisClient:\n    def __init__(self):\n        self.redis = None\n    \n    async def connect(self):\n        self.redis = redis.from_url(\n            settings.REDIS_URL,\n            encoding=\"utf-8\",\n            decode_responses=False\n        )\n    \n    async def disconnect(self):\n        if self.redis:\n            await self.redis.close()\n    \n    async def set_json(self, key: str, value: Any, expire: int = 3600):\n        await self.redis.set(key, json.dumps(value), ex=expire)\n    \n    async def get_json(self, key: str) -> Optional[Any]:\n        value = await self.redis.get(key)\n        return json.loads(value) if value else None\n    \n    async def set_pickle(self, key: str, value: Any, expire: int = 3600):\n        await self.redis.set(key, pickle.dumps(value), ex=expire)\n    \n    async def get_pickle(self, key: str) -> Optional[Any]:\n        value = await self.redis.get(key)\n        return pickle.loads(value) if value else None\n    \n    async def delete(self, key: str):\n        await self.redis.delete(key)\n    \n    async def exists(self, key: str) -> bool:\n        return await self.redis.exists(key)\n\nredis_client = RedisClient()\nEOF"
      ],
      "acceptance_criteria": [
        "Redis client configured",
        "Session management ready",
        "Caching utilities available",
        "Connection management implemented"
      ]
    },
    {
      "task_id": "01-013",
      "title": "Minio Object Storage Integration",
      "description": "Configure Minio S3-compatible object storage for file uploads, bucket management, file operations, and secure access with proper error handling",
      "estimated_time": "35 minutes",
      "estimated_tokens": 1950,
      "commands": [
        "mkdir -p src/services/storage",
        "cat > src/services/storage/minio_client.py << 'EOF'\nfrom minio import Minio\nfrom minio.error import S3Error\nfrom typing import BinaryIO, Optional, Dict, Any\nimport io\nfrom src.config.settings import get_settings\n\nsettings = get_settings()\n\nclass MinioClient:\n    def __init__(self):\n        self.client = Minio(\n            settings.MINIO_ENDPOINT,\n            access_key=settings.MINIO_ACCESS_KEY,\n            secret_key=settings.MINIO_SECRET_KEY,\n            secure=settings.MINIO_SECURE\n        )\n        self.default_bucket = \"ai-q-files\"\n    \n    async def ensure_bucket_exists(self, bucket_name: str):\n        try:\n            if not self.client.bucket_exists(bucket_name):\n                self.client.make_bucket(bucket_name)\n                # Set bucket policy for development\n                policy = {\n                    \"Version\": \"2012-10-17\",\n                    \"Statement\": [\n                        {\n                            \"Effect\": \"Allow\",\n                            \"Principal\": {\"AWS\": \"*\"},\n                            \"Action\": [\"s3:GetObject\"],\n                            \"Resource\": [f\"arn:aws:s3:::{bucket_name}/*\"]\n                        }\n                    ]\n                }\n                self.client.set_bucket_policy(bucket_name, json.dumps(policy))\n        except S3Error as e:\n            raise Exception(f\"Failed to create bucket {bucket_name}: {e}\")\n    \n    async def upload_file(self, bucket_name: str, object_name: str, file_data: BinaryIO, file_size: int, content_type: str = None) -> Dict[str, Any]:\n        try:\n            await self.ensure_bucket_exists(bucket_name)\n            result = self.client.put_object(\n                bucket_name,\n                object_name,\n                file_data,\n                file_size,\n                content_type=content_type\n            )\n            return {\n                \"bucket\": bucket_name,\n                \"object_name\": object_name,\n                \"etag\": result.etag,\n                \"size\": file_size\n            }\n        except S3Error as e:\n            raise Exception(f\"Failed to upload file: {e}\")\n    \n    async def download_file(self, bucket_name: str, object_name: str) -> bytes:\n        try:\n            response = self.client.get_object(bucket_name, object_name)\n            return response.read()\n        except S3Error as e:\n            raise Exception(f\"Failed to download file: {e}\")\n    \n    async def delete_file(self, bucket_name: str, object_name: str):\n        try:\n            self.client.remove_object(bucket_name, object_name)\n        except S3Error as e:\n            raise Exception(f\"Failed to delete file: {e}\")\n    \n    async def get_file_url(self, bucket_name: str, object_name: str, expires: int = 3600) -> str:\n        try:\n            return self.client.presigned_get_object(bucket_name, object_name, expires=expires)\n        except S3Error as e:\n            raise Exception(f\"Failed to generate URL: {e}\")\n\nminio_client = MinioClient()\nEOF"
      ],
      "acceptance_criteria": [
        "Minio client configured",
        "Bucket management implemented",
        "File upload/download working",
        "URL generation available"
      ]
    },
    {
      "task_id": "01-014",
      "title": "Weaviate Vector Database Integration",
      "description": "Configure Weaviate vector database with custom schemas for multi-modal data, embedding management, and semantic search capabilities",
      "estimated_time": "40 minutes",
      "estimated_tokens": 2100,
      "commands": [
        "mkdir -p src/services/vector",
        "cat > src/services/vector/weaviate_client.py << 'EOF'\nimport weaviate\nfrom typing import List, Dict, Any, Optional\nfrom src.config.settings import get_settings\n\nsettings = get_settings()\n\nclass WeaviateClient:\n    def __init__(self):\n        self.client = weaviate.Client(\n            url=settings.WEAVIATE_URL,\n            additional_headers={\n                \"X-OpenAI-Api-Key\": settings.OPENAI_API_KEY if hasattr(settings, 'OPENAI_API_KEY') else None\n            }\n        )\n        self.schemas = {\n            \"Document\": {\n                \"class\": \"Document\",\n                \"description\": \"Text documents and content\",\n                \"properties\": [\n                    {\"name\": \"content\", \"dataType\": [\"text\"], \"description\": \"Document content\"},\n                    {\"name\": \"filename\", \"dataType\": [\"string\"], \"description\": \"Original filename\"},\n                    {\"name\": \"file_id\", \"dataType\": [\"string\"], \"description\": \"Database file ID\"},\n                    {\"name\": \"chunk_index\", \"dataType\": [\"int\"], \"description\": \"Chunk position\"},\n                    {\"name\": \"metadata\", \"dataType\": [\"object\"], \"description\": \"Additional metadata\"}\n                ],\n                \"vectorizer\": \"text2vec-transformers\"\n            },\n            \"Image\": {\n                \"class\": \"Image\",\n                \"description\": \"Image files and visual content\",\n                \"properties\": [\n                    {\"name\": \"description\", \"dataType\": [\"text\"], \"description\": \"Image description\"},\n                    {\"name\": \"filename\", \"dataType\": [\"string\"], \"description\": \"Original filename\"},\n                    {\"name\": \"file_id\", \"dataType\": [\"string\"], \"description\": \"Database file ID\"},\n                    {\"name\": \"detected_objects\", \"dataType\": [\"string[]\"], \"description\": \"Detected objects\"},\n                    {\"name\": \"colors\", \"dataType\": [\"string[]\"], \"description\": \"Dominant colors\"},\n                    {\"name\": \"metadata\", \"dataType\": [\"object\"], \"description\": \"EXIF and other metadata\"}\n                ],\n                \"vectorizer\": \"img2vec-neural\"\n            }\n        }\n    \n    async def setup_schemas(self):\n        for schema_name, schema_def in self.schemas.items():\n            if not self.client.schema.exists(schema_name):\n                self.client.schema.create_class(schema_def)\n    \n    async def add_document(self, content: str, filename: str, file_id: str, chunk_index: int = 0, metadata: Dict = None) -> str:\n        try:\n            result = self.client.data_object.create(\n                data_object={\n                    \"content\": content,\n                    \"filename\": filename,\n                    \"file_id\": file_id,\n                    \"chunk_index\": chunk_index,\n                    \"metadata\": metadata or {}\n                },\n                class_name=\"Document\"\n            )\n            return result\n        except Exception as e:\n            raise Exception(f\"Failed to add document to Weaviate: {e}\")\n    \n    async def search_documents(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:\n        try:\n            result = (\n                self.client.query\n                .get(\"Document\", [\"content\", \"filename\", \"file_id\", \"chunk_index\", \"metadata\"])\n                .with_near_text({\"concepts\": [query]})\n                .with_limit(limit)\n                .with_additional([\"certainty\"])\n                .do()\n            )\n            return result[\"data\"][\"Get\"][\"Document\"]\n        except Exception as e:\n            raise Exception(f\"Failed to search documents: {e}\")\n    \n    async def delete_by_file_id(self, file_id: str):\n        try:\n            self.client.batch.delete_objects(\n                class_name=\"Document\",\n                where={\n                    \"path\": [\"file_id\"],\n                    \"operator\": \"Equal\",\n                    \"valueString\": file_id\n                }\n            )\n        except Exception as e:\n            raise Exception(f\"Failed to delete documents: {e}\")\n\nweaviate_client = WeaviateClient()\nEOF"
      ],
      "acceptance_criteria": [
        "Weaviate client configured",
        "Multi-modal schemas created",
        "Embedding operations working",
        "Search functionality available"
      ]
    },
    {
      "task_id": "01-015",
      "title": "Docker Container Build Configuration",
      "description": "Create Dockerfiles for Python FastAPI backend and Node.js React frontend with multi-stage builds, optimization, and security best practices",
      "estimated_time": "30 minutes",
      "estimated_tokens": 1800,
      "commands": [
        "mkdir -p docker/{python,nodejs}",
        "cat > docker/python/Dockerfile << 'EOF'\n# Multi-stage build for Python FastAPI backend\nFROM python:3.10-slim as builder\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    gcc \\\n    g++ \\\n    libffi-dev \\\n    libssl-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements and install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --user -r requirements.txt\n\n# Production stage\nFROM python:3.10-slim\n\nWORKDIR /app\n\n# Create non-root user\nRUN groupadd -r appuser && useradd -r -g appuser appuser\n\n# Copy Python packages from builder\nCOPY --from=builder /root/.local /home/appuser/.local\n\n# Copy application code\nCOPY src/ ./src/\nCOPY config/ ./config/\n\n# Set environment variables\nENV PATH=/home/appuser/.local/bin:$PATH\nENV PYTHONPATH=/app\nENV PYTHONUNBUFFERED=1\n\n# Change ownership and switch to non-root user\nRUN chown -R appuser:appuser /app\nUSER appuser\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Start application\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\nEOF",
        "cat > docker/nodejs/Dockerfile << 'EOF'\n# Multi-stage build for React frontend\nFROM node:18-alpine as builder\n\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy source code\nCOPY public/ ./public/\nCOPY src/ ./src/\nCOPY tsconfig.json ./\n\n# Build application\nRUN npm run build\n\n# Production stage with nginx\nFROM nginx:alpine\n\n# Copy built application\nCOPY --from=builder /app/build /usr/share/nginx/html/\n\n# Copy nginx configuration\nCOPY docker/nodejs/nginx.conf /etc/nginx/conf.d/default.conf\n\n# Expose port\nEXPOSE 80\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost/ || exit 1\n\n# Start nginx\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\nEOF"
      ],
      "acceptance_criteria": [
        "Python Dockerfile created",
        "Node.js Dockerfile created",
        "Multi-stage builds configured",
        "Security best practices followed"
      ]
    }
  ]
} 