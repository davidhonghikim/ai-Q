{
  "metadata": {
    "title": "AI Services Feature Flags",
    "version": "1.0.0",
    "created_by": "Q-Assist",
    "created_at": "2025-07-09T08:20:00Z",
    "purpose": "Control which AI services are used for vectorization and generation"
  },
  "vectorization_service": {
    "primary": {
      "service": "huggingface-transformers",
      "enabled": true,
      "description": "High-quality sentence transformers for text vectorization",
      "endpoint": "http://huggingface-inference:8080",
      "model": "sentence-transformers/all-MiniLM-L6-v2",
      "fallback": "ollama-embeddings"
    },
    "fallback": {
      "service": "ollama-embeddings",
      "enabled": true,
      "description": "Ollama's embedding models as backup vectorization",
      "endpoint": "http://ollama:11434",
      "model": "nomic-embed-text",
      "fallback": "none"
    }
  },
  "generation_service": {
    "primary": {
      "service": "ollama",
      "enabled": true,
      "description": "Ollama LLM for text generation and chat",
      "endpoint": "http://ollama:11434",
      "model": "llama3.2:3b",
      "fallback": "huggingface-generation"
    },
    "fallback": {
      "service": "huggingface-generation",
      "enabled": false,
      "description": "HuggingFace generative models as backup",
      "endpoint": "http://huggingface-inference:8080",
      "model": "microsoft/DialoGPT-medium",
      "fallback": "none"
    }
  },
  "service_switching": {
    "auto_fallback": true,
    "health_check_interval": 30,
    "retry_attempts": 3,
    "switch_on_failure": true
  },
  "performance_settings": {
    "huggingface": {
      "max_batch_size": 32,
      "max_batch_wait": 0.1,
      "gpu_memory_fraction": 0.8
    },
    "ollama": {
      "max_concurrent_requests": 10,
      "timeout_seconds": 30,
      "gpu_memory_fraction": 0.9
    }
  }
} 