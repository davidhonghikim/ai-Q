@echo off
REM AI-Q Ollama Model Puller (Windows)
REM This script pulls the default models for the AI-Q system

echo ğŸš€ AI-Q Ollama Model Puller
echo ==========================

REM Default models to pull (smallest/fastest for quick startup)
set MODELS=gemma3:2b llama3.2:3b

REM Optional larger models (uncomment to include)
REM set MODELS=%MODELS% codellama:7b qwen2.5:7b

REM Check if Ollama is running
curl -s http://localhost:11434/api/tags >nul 2>&1
if errorlevel 1 (
    echo âŒ Error: Ollama is not running on localhost:11434
    echo Please start Ollama first: docker-compose up ollama
    pause
    exit /b 1
)

echo âœ… Ollama is running
echo ğŸ“¥ Pulling default models...

REM Pull each model
for %%m in (%MODELS%) do (
    echo ğŸ“¦ Pulling %%m...
    ollama pull %%m
    if errorlevel 1 (
        echo âŒ Failed to pull %%m
    ) else (
        echo âœ… Successfully pulled %%m
    )
    echo.
)

echo ğŸ‰ Model pulling complete!
echo.
echo ğŸ“‹ Available models:
ollama list

echo.
echo ğŸ’¡ Model recommendations:
echo   â€¢ Function Calling: llama3.2:3b (default), qwen2.5:7b (optional), codellama:7b (optional)
echo   â€¢ Code Generation: codellama:7b (optional), llama3.2:3b (default)
echo   â€¢ General Tasks: gemma3:2b (default), qwen2.5:7b (optional)
echo   â€¢ Fast Response: gemma3:2b (default), llama3.2:3b (default)
echo   â€¢ High Quality: qwen2.5:7b (optional), codellama:7b (optional)

pause 