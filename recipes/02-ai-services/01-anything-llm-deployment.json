{
  "recipe_metadata": {
    "recipe_id": "04-AI-SERVICE-ANYTHING-LLM-001",
    "title": "Deploy AnythingLLM for RAG and Agent Management",
    "version": "1.0.0",
    "created_by": "Gemini",
    "creation_date": "2025-07-05T00:00:00Z",
    "last_updated": "2025-07-05T00:00:00Z",
    "estimated_tokens": 9000,
    "estimated_execution_time": "2-3 hours",
    "difficulty_level": "advanced",
    "total_tasks": 1,
    "agent_autonomy_level": "95%",
    "success_rate_target": "99%",
    "compliance_standards": [],
    "architecture_tier": "ai-application"
  },
  "recipe_overview": {
    "description": "Deploy the AnythingLLM application, an all-in-one AI platform for Retrieval-Augmented Generation (RAG), AI agent creation, and vector database management. This recipe provides instructions for running AnythingLLM via Docker and connecting it to the Weaviate vector database deployed in the main infrastructure.",
    "technology_stack": {
      "application": "AnythingLLM",
      "vector_database": "Weaviate",
      "llm": "Ollama (or other configured LLM)",
      "orchestration": "Docker Compose"
    },
    "deliverables": [
      "A running instance of AnythingLLM",
      "Configuration connecting AnythingLLM to the project's Weaviate and LLM services",
      "Documentation on how to access and use the AnythingLLM interface"
    ],
    "integration_source": "https://github.com/Mintplex-Labs/anything-llm"
  },
  "tasks": [
    {
      "id": "04-ai-service-anything-llm-001",
      "title": "Deploy AnythingLLM Docker Container",
      "description": "Deploy the AnythingLLM application, an all-in-one AI platform for Retrieval-Augmented Generation (RAG), AI agent creation, and vector database management. This recipe provides instructions for running AnythingLLM via Docker and connecting it to the Weaviate vector database deployed in the main infrastructure.",
      "category": "ai-services",
      "estimated_tokens": 9000,
      "estimated_duration": "2-3 hours",
      "difficulty_level": "advanced",
      "prerequisites": {
        "knowledge_required": ["AnythingLLM", "Docker", "Vector Databases", "LLM Configuration"],
        "tools_required": ["Docker", "Docker Compose"],
        "environment_setup": ["A running Weaviate instance", "An accessible LLM (e.g., Ollama)"]
      },
      "inputs": {
        "files_to_read": ["docker-compose.anythingllm.yml"],
        "config_dependencies": ["Weaviate API endpoint and API key", "LLM API endpoint"],
        "environment_variables": ["WEAVIATE_ENDPOINT", "WEAVIATE_API_KEY", "LLM_PROVIDER_ENDPOINT"]
      },
      "outputs": {
        "files_created": [
          "docker-compose.anythingllm.yml - Docker Compose file for AnythingLLM",
          "docs/ai-services/anything-llm-guide.md - Guide on using the deployed instance"
        ],
        "files_modified": [],
        "api_endpoints": [
          "http://localhost:3001 - AnythingLLM web interface"
        ]
      },
      "dependencies": {
        "required_tasks": [
          "03-database-weaviate-cluster-004"
        ]
      },
      "detailed_instructions": {
        "overview": "This task involves creating a Docker Compose file to deploy AnythingLLM. The configuration will mount a volume for storage and pass environment variables to connect to the required Weaviate and LLM backend services. This approach makes AnythingLLM an optional, containerized component of the system.",
        "step_by_step_guide": [
          {
            "step": 1,
            "title": "Create AnythingLLM Docker Compose File",
            "description": "Create a `docker-compose.anythingllm.yml` file to define the service.",
            "commands": [
              "cat > docker-compose.anythingllm.yml << 'EOF'",
              "version: '3.8'",
              "services:",
              "  anythingllm:",
              "    image: mintplexlabs/anythingllm:latest",
              "    container_name: anythingllm",
              "    ports:",
              "      - '3001:3001'",
              "    environment:",
              "      - STORAGE_LOCATION=/app/server/storage",
              "    volumes:",
              "      - anythingllm_storage:/app/server/storage",
              "      - anythingllm_collector:/app/collector/hotdir",
              "    cap_add:",
              "      - SYS_ADMIN",
              "    devices:",
              "      - /dev/fuse:/dev/fuse",
              "    security_opt:",
              "      - apparmor:unconfined",
              "volumes:",
              "  anythingllm_storage:",
              "  anythingllm_collector:",
              "EOF"
            ],
            "expected_output": "`docker-compose.anythingllm.yml` file is created.",
            "troubleshooting": "Refer to the official AnythingLLM Docker guide for the latest image tags and configuration options."
          },
          {
            "step": 2,
            "title": "Launch AnythingLLM",
            "description": "Start the AnythingLLM container using Docker Compose.",
            "commands": [
              "docker-compose -f docker-compose.anythingllm.yml up -d"
            ],
            "expected_output": "The AnythingLLM container is running and accessible on port 3001.",
            "troubleshooting": "Check container logs (`docker logs anythingllm`) for startup errors."
          },
          {
            "step": 3,
            "title": "Configure AnythingLLM",
            "description": "Open the AnythingLLM web UI at http://localhost:3001 and perform the initial setup.",
            "commands": [
              "# Manual Steps:",
              "# 1. Navigate to http://localhost:3001",
              "# 2. Select your desired LLM provider (e.g., Ollama).",
              "# 3. Enter the LLM Base URL (e.g., http://host.docker.internal:11434).",
              "# 4. Select Weaviate as the vector database.",
              "# 5. Enter the Weaviate endpoint (e.g., http://host.docker.internal:8080).",
              "# 6. Enter the Weaviate API Key.",
              "# 7. Complete the setup wizard."
            ],
            "expected_output": "AnythingLLM is configured to use the project's backend services.",
            "troubleshooting": "Ensure network connectivity from the AnythingLLM container to the Weaviate and LLM containers. `host.docker.internal` is often used to allow containers to reach the host machine."
          }
        ]
      },
      "acceptance_criteria": {
        "functional_requirements": [
          "AnythingLLM is deployed and accessible.",
          "It is successfully connected to the Weaviate and LLM services.",
          "Users can create workspaces and add documents for RAG."
        ]
      }
    }
  ]
} 