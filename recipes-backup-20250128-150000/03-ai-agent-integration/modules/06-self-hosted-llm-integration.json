{
  "recipe_metadata": {
    "recipe_id": "03-AI-AGENT-LLM-INTEGRATION-006",
    "title": "Self-Hosted LLM Integration (Ollama)",
    "version": "1.0.0",
    "created_by": "Gemini",
    "creation_date": "2025-07-05T00:00:00Z",
    "last_updated": "2025-07-05T00:00:00Z",
    "difficulty_level": "expert",
    "total_tasks": 1
  },
  "recipe_overview": {
    "description": "Integrate Ollama and local LLM models to provide self-hosted language model capabilities for all AI agent components, ensuring privacy and customization.",
    "technology_stack": {
      "llm_server": "Ollama",
      "orchestration": "Docker Compose",
      "language": "TypeScript"
    },
    "deliverables": [
      "A running Ollama container",
      "Ability to load and use local LLM models",
      "A unified inference engine for all AI agents to use"
    ]
  },
  "tasks": [
    {
      "id": "36-006",
      "title": "Self-Hosted LLM Integration",
      "description": "Integrate Ollama and local LLM models for all AI agent components",
      "estimated_time": "60 minutes",
      "estimated_tokens": 3000,
      "commands": [
        "docker pull ollama/ollama:latest",
        "mkdir -p config/llm-models",
        "mkdir -p data/llm-cache"
      ],
      "files_to_create": [
        "src/modules/ai-agents/llm/ollama-manager.ts",
        "src/modules/ai-agents/llm/model-loader.ts",
        "src/modules/ai-agents/llm/inference-engine.ts",
        "src/modules/ai-agents/llm/cache-manager.ts",
        "config/llm-models/models.json",
        "docker-compose.llm.yml"
      ],
      "acceptance_criteria": [
        "Ollama container running and accessible",
        "Local models can be loaded and used",
        "Inference engine working with all AI agents"
      ]
    }
  ]
} 