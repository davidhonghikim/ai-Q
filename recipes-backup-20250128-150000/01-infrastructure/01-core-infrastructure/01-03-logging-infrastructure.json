{
  "metadata": {
    "sub_recipe_id": "01-03-logging-infrastructure",
    "title": "Centralized Logging System - Complete Implementation",
    "version": "1.0.0",
    "created_by": "Claude Sonnet 4",
    "last_updated": "2025-01-27T16:30:00Z",
    "purpose": "Exact step-by-step ELK stack logging infrastructure implementation with zero ambiguity",
    "total_tasks": 18,
    "estimated_duration": "2-3 days",
    "complexity": "Advanced",
    "dependencies": ["01-01-docker-environment", "01-02-system-monitoring"],
    "logging_stack": ["Elasticsearch", "Logstash", "Kibana", "Filebeat", "Fluentd"]
  },
  "prerequisites": {
    "completed_tasks": [
      "01-01-01: Docker Engine installed",
      "01-01-02: Docker daemon configured",
      "01-01-03: Docker networks created",
      "01-02-01: Prometheus installed",
      "01-02-02: Node Exporter running",
      "01-02-03: Alert Manager configured",
      "01-02-04: Grafana installed",
      "01-02-05: Monitoring stack running"
    ],
    "system_requirements": {
      "cpu": "Minimum 4 cores for ELK stack",
      "ram": "Minimum 8GB for ELK stack (4GB for Elasticsearch alone)",
      "storage": "Minimum 50GB for log storage",
      "network": "Access to logging networks"
    }
  },
  "task_01_install_elasticsearch": {
    "task_id": "01-03-01",
    "title": "Install and Configure Elasticsearch",
    "description": "Install Elasticsearch with exact configuration and security settings",
    "estimated_duration": "60 minutes",
    "critical": true,
    "steps": [
      {
        "step_id": "01-03-01-01",
        "title": "Create Elasticsearch configuration directory",
        "description": "Create directory structure for Elasticsearch",
        "commands": [
          "sudo mkdir -p /opt/ai-q/logging/elasticsearch/config",
          "sudo mkdir -p /opt/ai-q/logging/elasticsearch/data",
          "sudo mkdir -p /opt/ai-q/logging/elasticsearch/logs",
          "sudo mkdir -p /opt/ai-q/logging/elasticsearch/plugins"
        ],
        "verification": "Check directories created",
        "expected_output": "All Elasticsearch directories created successfully"
      },
      {
        "step_id": "01-03-01-02",
        "title": "Create Elasticsearch configuration file",
        "description": "Create elasticsearch.yml with exact configuration",
        "config_file": "/opt/ai-q/logging/elasticsearch/config/elasticsearch.yml",
        "config_content": {
          "cluster.name": "ai-q-cluster",
          "node.name": "ai-q-node-1",
          "path.data": "/usr/share/elasticsearch/data",
          "path.logs": "/usr/share/elasticsearch/logs",
          "network.host": "0.0.0.0",
          "http.port": 9200,
          "discovery.type": "single-node",
          "xpack.security.enabled": false,
          "xpack.monitoring.enabled": true,
          "xpack.watcher.enabled": false,
          "cluster.routing.allocation.disk.threshold_enabled": true,
          "cluster.routing.allocation.disk.watermark.low": "93%",
          "cluster.routing.allocation.disk.watermark.high": "95%",
          "cluster.routing.allocation.disk.watermark.flood_stage": "97%"
        },
        "commands": [
          "sudo tee /opt/ai-q/logging/elasticsearch/config/elasticsearch.yml << 'EOF'\ncluster.name: ai-q-cluster\nnode.name: ai-q-node-1\npath.data: /usr/share/elasticsearch/data\npath.logs: /usr/share/elasticsearch/logs\nnetwork.host: 0.0.0.0\nhttp.port: 9200\ndiscovery.type: single-node\nxpack.security.enabled: false\nxpack.monitoring.enabled: true\nxpack.watcher.enabled: false\ncluster.routing.allocation.disk.threshold_enabled: true\ncluster.routing.allocation.disk.watermark.low: 93%\ncluster.routing.allocation.disk.watermark.high: 95%\ncluster.routing.allocation.disk.watermark.flood_stage: 97%\nEOF"
        ],
        "verification": "Check configuration file exists",
        "expected_output": "elasticsearch.yml created with specified configuration"
      },
      {
        "step_id": "01-03-01-03",
        "title": "Create JVM options configuration",
        "description": "Create jvm.options with memory and performance settings",
        "config_file": "/opt/ai-q/logging/elasticsearch/config/jvm.options",
        "config_content": [
          "-Xms2g",
          "-Xmx2g",
          "-XX:+UseG1GC",
          "-XX:G1ReservePercent=25",
          "-XX:InitiatingHeapOccupancyPercent=75",
          "-XX:+HeapDumpOnOutOfMemoryError",
          "-XX:HeapDumpPath=data",
          "-XX:ErrorFile=logs/hs_err_pid%p.log",
          "-XX:+PrintGCDetails",
          "-XX:+PrintGCTimeStamps",
          "-XX:+PrintGCDateStamps",
          "-XX:+PrintTenuringDistribution",
          "-XX:+PrintGCApplicationStoppedTime",
          "-Xloggc:logs/gc.log",
          "-XX:+UseGCLogFileRotation",
          "-XX:NumberOfGCLogFiles=32",
          "-XX:GCLogFileSize=64m"
        ],
        "commands": [
          "sudo tee /opt/ai-q/logging/elasticsearch/config/jvm.options << 'EOF'\n-Xms2g\n-Xmx2g\n-XX:+UseG1GC\n-XX:G1ReservePercent=25\n-XX:InitiatingHeapOccupancyPercent=75\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=data\n-XX:ErrorFile=logs/hs_err_pid%p.log\n-XX:+PrintGCDetails\n-XX:+PrintGCTimeStamps\n-XX:+PrintGCDateStamps\n-XX:+PrintTenuringDistribution\n-XX:+PrintGCApplicationStoppedTime\n-Xloggc:logs/gc.log\n-XX:+UseGCLogFileRotation\n-XX:NumberOfGCLogFiles=32\n-XX:GCLogFileSize=64m\nEOF"
        ],
        "verification": "Check JVM options file exists",
        "expected_output": "jvm.options created with specified configuration"
      },
      {
        "step_id": "01-03-01-04",
        "title": "Set Elasticsearch permissions",
        "description": "Set correct permissions for Elasticsearch",
        "commands": [
          "sudo chown -R 1000:1000 /opt/ai-q/logging/elasticsearch/",
          "sudo chmod -R 755 /opt/ai-q/logging/elasticsearch/"
        ],
        "verification": "Check permissions set correctly",
        "expected_output": "Elasticsearch directories have correct ownership (1000:1000)"
      }
    ],
    "verification_commands": [
      "ls -la /opt/ai-q/logging/elasticsearch/",
      "cat /opt/ai-q/logging/elasticsearch/config/elasticsearch.yml"
    ],
    "expected_outputs": {
      "config_file": "elasticsearch.yml contains specified configuration",
      "jvm_options": "jvm.options contains memory settings",
      "permissions": "Files owned by user 1000:1000"
    }
  },
  "task_02_install_logstash": {
    "task_id": "01-03-02",
    "title": "Install and Configure Logstash",
    "description": "Install Logstash with pipeline configuration for log processing",
    "estimated_duration": "45 minutes",
    "critical": true,
    "steps": [
      {
        "step_id": "01-03-02-01",
        "title": "Create Logstash configuration directory",
        "description": "Create directory structure for Logstash",
        "commands": [
          "sudo mkdir -p /opt/ai-q/logging/logstash/config",
          "sudo mkdir -p /opt/ai-q/logging/logstash/pipeline",
          "sudo mkdir -p /opt/ai-q/logging/logstash/logs",
          "sudo mkdir -p /opt/ai-q/logging/logstash/patterns"
        ],
        "verification": "Check directories created",
        "expected_output": "All Logstash directories created successfully"
      },
      {
        "step_id": "01-03-02-02",
        "title": "Create Logstash configuration file",
        "description": "Create logstash.yml with exact configuration",
        "config_file": "/opt/ai-q/logging/logstash/config/logstash.yml",
        "config_content": {
          "http.host": "0.0.0.0",
          "xpack.monitoring.elasticsearch.hosts": ["http://elasticsearch:9200"],
          "xpack.monitoring.enabled": true,
          "pipeline.workers": 2,
          "pipeline.batch.size": 125,
          "pipeline.batch.delay": 50,
          "queue.type": "memory",
          "queue.max_events": 1000
        },
        "commands": [
          "sudo tee /opt/ai-q/logging/logstash/config/logstash.yml << 'EOF'\nhttp.host: 0.0.0.0\nxpack.monitoring.elasticsearch.hosts: [\"http://elasticsearch:9200\"]\nxpack.monitoring.enabled: true\npipeline.workers: 2\npipeline.batch.size: 125\npipeline.batch.delay: 50\nqueue.type: memory\nqueue.max_events: 1000\nEOF"
        ],
        "verification": "Check configuration file exists",
        "expected_output": "logstash.yml created with specified configuration"
      },
      {
        "step_id": "01-03-02-03",
        "title": "Create main pipeline configuration",
        "description": "Create main pipeline for log processing",
        "config_file": "/opt/ai-q/logging/logstash/pipeline/main.conf",
        "config_content": {
          "input": {
            "beats": {
              "port": 5044,
              "host": "0.0.0.0"
            }
          },
          "filter": {
            "if": {
              "[fields][service]": "docker"
            },
            "docker": {},
            "grok": {
              "match": {
                "message": "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}"
              }
            },
            "date": {
              "match": ["timestamp", "ISO8601"]
            }
          },
          "output": {
            "elasticsearch": {
              "hosts": ["elasticsearch:9200"],
              "index": "ai-q-logs-%{+YYYY.MM.dd}"
            }
          }
        },
        "commands": [
          "sudo tee /opt/ai-q/logging/logstash/pipeline/main.conf << 'EOF'\ninput {\n  beats {\n    port => 5044\n    host => \"0.0.0.0\"\n  }\n}\n\nfilter {\n  if [fields][service] == \"docker\" {\n    docker {}\n  }\n  \n  grok {\n    match => { \"message\" => \"%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}\" }\n  }\n  \n  date {\n    match => [ \"timestamp\", \"ISO8601\" ]\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"elasticsearch:9200\"]\n    index => \"ai-q-logs-%{+YYYY.MM.dd}\"\n  }\n}\nEOF"
        ],
        "verification": "Check pipeline configuration exists",
        "expected_output": "main.conf created with specified configuration"
      },
      {
        "step_id": "01-03-02-04",
        "title": "Set Logstash permissions",
        "description": "Set correct permissions for Logstash",
        "commands": [
          "sudo chown -R 1000:1000 /opt/ai-q/logging/logstash/",
          "sudo chmod -R 755 /opt/ai-q/logging/logstash/"
        ],
        "verification": "Check permissions set correctly",
        "expected_output": "Logstash directories have correct ownership (1000:1000)"
      }
    ],
    "verification_commands": [
      "ls -la /opt/ai-q/logging/logstash/",
      "cat /opt/ai-q/logging/logstash/config/logstash.yml"
    ],
    "expected_outputs": {
      "config_file": "logstash.yml contains specified configuration",
      "pipeline": "main.conf contains pipeline configuration",
      "permissions": "Files owned by user 1000:1000"
    }
  },
  "task_03_install_kibana": {
    "task_id": "01-03-03",
    "title": "Install and Configure Kibana",
    "description": "Install Kibana with exact configuration for log visualization",
    "estimated_duration": "30 minutes",
    "critical": true,
    "steps": [
      {
        "step_id": "01-03-03-01",
        "title": "Create Kibana configuration directory",
        "description": "Create directory structure for Kibana",
        "commands": [
          "sudo mkdir -p /opt/ai-q/logging/kibana/config",
          "sudo mkdir -p /opt/ai-q/logging/kibana/data"
        ],
        "verification": "Check directories created",
        "expected_output": "All Kibana directories created successfully"
      },
      {
        "step_id": "01-03-03-02",
        "title": "Create Kibana configuration file",
        "description": "Create kibana.yml with exact configuration",
        "config_file": "/opt/ai-q/logging/kibana/config/kibana.yml",
        "config_content": {
          "server.name": "ai-q-kibana",
          "server.host": "0.0.0.0",
          "server.port": 5601,
          "elasticsearch.hosts": ["http://elasticsearch:9200"],
          "elasticsearch.username": "kibana_system",
          "elasticsearch.password": "changeme",
          "xpack.security.enabled": false,
          "xpack.monitoring.enabled": true,
          "xpack.reporting.enabled": false,
          "xpack.encryptedSavedObjects.encryptionKey": "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
          "logging.dest": "stdout",
          "logging.level": "info"
        },
        "commands": [
          "sudo tee /opt/ai-q/logging/kibana/config/kibana.yml << 'EOF'\nserver.name: ai-q-kibana\nserver.host: 0.0.0.0\nserver.port: 5601\nelasticsearch.hosts: [\"http://elasticsearch:9200\"]\nelasticsearch.username: kibana_system\nelasticsearch.password: changeme\nxpack.security.enabled: false\nxpack.monitoring.enabled: true\nxpack.reporting.enabled: false\nxpack.encryptedSavedObjects.encryptionKey: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nlogging.dest: stdout\nlogging.level: info\nEOF"
        ],
        "verification": "Check configuration file exists",
        "expected_output": "kibana.yml created with specified configuration"
      },
      {
        "step_id": "01-03-03-03",
        "title": "Set Kibana permissions",
        "description": "Set correct permissions for Kibana",
        "commands": [
          "sudo chown -R 1000:1000 /opt/ai-q/logging/kibana/",
          "sudo chmod -R 755 /opt/ai-q/logging/kibana/"
        ],
        "verification": "Check permissions set correctly",
        "expected_output": "Kibana directories have correct ownership (1000:1000)"
      }
    ],
    "verification_commands": [
      "ls -la /opt/ai-q/logging/kibana/",
      "cat /opt/ai-q/logging/kibana/config/kibana.yml"
    ],
    "expected_outputs": {
      "config_file": "kibana.yml contains specified configuration",
      "permissions": "Files owned by user 1000:1000"
    }
  },
  "task_04_install_filebeat": {
    "task_id": "01-03-04",
    "title": "Install and Configure Filebeat",
    "description": "Install Filebeat for log collection from Docker containers",
    "estimated_duration": "25 minutes",
    "critical": true,
    "steps": [
      {
        "step_id": "01-03-04-01",
        "title": "Create Filebeat configuration directory",
        "description": "Create directory structure for Filebeat",
        "commands": [
          "sudo mkdir -p /opt/ai-q/logging/filebeat/config",
          "sudo mkdir -p /opt/ai-q/logging/filebeat/data",
          "sudo mkdir -p /opt/ai-q/logging/filebeat/logs"
        ],
        "verification": "Check directories created",
        "expected_output": "All Filebeat directories created successfully"
      },
      {
        "step_id": "01-03-04-02",
        "title": "Create Filebeat configuration file",
        "description": "Create filebeat.yml with exact configuration",
        "config_file": "/opt/ai-q/logging/filebeat/config/filebeat.yml",
        "config_content": {
          "filebeat.inputs": [
            {
              "type": "container",
              "paths": ["/var/lib/docker/containers/*/*.log"],
              "processors": [
                {
                  "add_docker_metadata": {
                    "host": "unix:///var/run/docker.sock"
                  }
                }
              ]
            }
          ],
          "processors": [
            {
              "add_fields": {
                "fields": {
                  "service": "docker"
                }
              }
            }
          ],
          "output.logstash": {
            "hosts": ["logstash:5044"]
          },
          "logging.level": "info",
          "logging.to_files": true,
          "logging.files.path": "/var/log/filebeat",
          "logging.files.name": "filebeat",
          "logging.files.keepfiles": 7,
          "logging.files.permissions": "0644"
        },
        "commands": [
          "sudo tee /opt/ai-q/logging/filebeat/config/filebeat.yml << 'EOF'\nfilebeat.inputs:\n- type: container\n  paths:\n    - /var/lib/docker/containers/*/*.log\n  processors:\n    - add_docker_metadata:\n        host: unix:///var/run/docker.sock\n\nprocessors:\n- add_fields:\n    fields:\n      service: docker\n\noutput.logstash:\n  hosts: [\"logstash:5044\"]\n\nlogging.level: info\nlogging.to_files: true\nlogging.files.path: /var/log/filebeat\nlogging.files.name: filebeat\nlogging.files.keepfiles: 7\nlogging.files.permissions: 0644\nEOF"
        ],
        "verification": "Check configuration file exists",
        "expected_output": "filebeat.yml created with specified configuration"
      },
      {
        "step_id": "01-03-04-03",
        "title": "Set Filebeat permissions",
        "description": "Set correct permissions for Filebeat",
        "commands": [
          "sudo chown -R 1000:1000 /opt/ai-q/logging/filebeat/",
          "sudo chmod -R 755 /opt/ai-q/logging/filebeat/"
        ],
        "verification": "Check permissions set correctly",
        "expected_output": "Filebeat directories have correct ownership (1000:1000)"
      }
    ],
    "verification_commands": [
      "ls -la /opt/ai-q/logging/filebeat/",
      "cat /opt/ai-q/logging/filebeat/config/filebeat.yml"
    ],
    "expected_outputs": {
      "config_file": "filebeat.yml contains specified configuration",
      "permissions": "Files owned by user 1000:1000"
    }
  },
  "task_05_create_logging_docker_compose": {
    "task_id": "01-03-05",
    "title": "Create Docker Compose for Logging Stack",
    "description": "Create docker-compose.yml for all logging services",
    "estimated_duration": "30 minutes",
    "critical": true,
    "steps": [
      {
        "step_id": "01-03-05-01",
        "title": "Create logging docker-compose.yml",
        "description": "Create complete docker-compose.yml for logging stack",
        "config_file": "/opt/ai-q/docker/compose/logging-stack.yml",
        "config_content": {
          "version": "3.8",
          "services": {
            "elasticsearch": {
              "image": "docker.elastic.co/elasticsearch/elasticsearch:8.11.0",
              "container_name": "ai-q-elasticsearch",
              "ports": ["9200:9200", "9300:9300"],
              "volumes": [
                "/opt/ai-q/logging/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro",
                "/opt/ai-q/logging/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options:ro",
                "/opt/ai-q/logging/elasticsearch/data:/usr/share/elasticsearch/data",
                "/opt/ai-q/logging/elasticsearch/logs:/usr/share/elasticsearch/logs"
              ],
              "environment": [
                "discovery.type=single-node",
                "ES_JAVA_OPTS=-Xms2g -Xmx2g"
              ],
              "networks": ["ai-q-monitoring"],
              "restart": "unless-stopped"
            },
            "logstash": {
              "image": "docker.elastic.co/logstash/logstash:8.11.0",
              "container_name": "ai-q-logstash",
              "ports": ["5044:5044", "9600:9600"],
              "volumes": [
                "/opt/ai-q/logging/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro",
                "/opt/ai-q/logging/logstash/pipeline:/usr/share/logstash/pipeline:ro",
                "/opt/ai-q/logging/logstash/logs:/usr/share/logstash/logs"
              ],
              "environment": [
                "LS_JAVA_OPTS=-Xms1g -Xmx1g"
              ],
              "networks": ["ai-q-monitoring"],
              "depends_on": ["elasticsearch"],
              "restart": "unless-stopped"
            },
            "kibana": {
              "image": "docker.elastic.co/kibana/kibana:8.11.0",
              "container_name": "ai-q-kibana",
              "ports": ["5601:5601"],
              "volumes": [
                "/opt/ai-q/logging/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro",
                "/opt/ai-q/logging/kibana/data:/usr/share/kibana/data"
              ],
              "environment": [
                "ELASTICSEARCH_HOSTS=http://elasticsearch:9200"
              ],
              "networks": ["ai-q-monitoring"],
              "depends_on": ["elasticsearch"],
              "restart": "unless-stopped"
            },
            "filebeat": {
              "image": "docker.elastic.co/beats/filebeat:8.11.0",
              "container_name": "ai-q-filebeat",
              "volumes": [
                "/opt/ai-q/logging/filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro",
                "/var/lib/docker/containers:/var/lib/docker/containers:ro",
                "/var/run/docker.sock:/var/run/docker.sock:ro"
              ],
              "user": "root",
              "networks": ["ai-q-monitoring"],
              "depends_on": ["logstash"],
              "restart": "unless-stopped"
            }
          },
          "networks": {
            "ai-q-monitoring": {
              "external": true
            }
          }
        },
        "commands": [
          "sudo tee /opt/ai-q/docker/compose/logging-stack.yml << 'EOF'\nversion: '3.8'\n\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n    container_name: ai-q-elasticsearch\n    ports:\n      - 9200:9200\n      - 9300:9300\n    volumes:\n      - /opt/ai-q/logging/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro\n      - /opt/ai-q/logging/elasticsearch/config/jvm.options:/usr/share/elasticsearch/config/jvm.options:ro\n      - /opt/ai-q/logging/elasticsearch/data:/usr/share/elasticsearch/data\n      - /opt/ai-q/logging/elasticsearch/logs:/usr/share/elasticsearch/logs\n    environment:\n      - discovery.type=single-node\n      - ES_JAVA_OPTS=-Xms2g -Xmx2g\n    networks:\n      - ai-q-monitoring\n    restart: unless-stopped\n\n  logstash:\n    image: docker.elastic.co/logstash/logstash:8.11.0\n    container_name: ai-q-logstash\n    ports:\n      - 5044:5044\n      - 9600:9600\n    volumes:\n      - /opt/ai-q/logging/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro\n      - /opt/ai-q/logging/logstash/pipeline:/usr/share/logstash/pipeline:ro\n      - /opt/ai-q/logging/logstash/logs:/usr/share/logstash/logs\n    environment:\n      - LS_JAVA_OPTS=-Xms1g -Xmx1g\n    networks:\n      - ai-q-monitoring\n    depends_on:\n      - elasticsearch\n    restart: unless-stopped\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.11.0\n    container_name: ai-q-kibana\n    ports:\n      - 5601:5601\n    volumes:\n      - /opt/ai-q/logging/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro\n      - /opt/ai-q/logging/kibana/data:/usr/share/kibana/data\n    environment:\n      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200\n    networks:\n      - ai-q-monitoring\n    depends_on:\n      - elasticsearch\n    restart: unless-stopped\n\n  filebeat:\n    image: docker.elastic.co/beats/filebeat:8.11.0\n    container_name: ai-q-filebeat\n    volumes:\n      - /opt/ai-q/logging/filebeat/config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    user: root\n    networks:\n      - ai-q-monitoring\n    depends_on:\n      - logstash\n    restart: unless-stopped\n\nnetworks:\n  ai-q-monitoring:\n    external: true\nEOF"
        ],
        "verification": "Check docker-compose file exists",
        "expected_output": "logging-stack.yml created with specified configuration"
      }
    ],
    "verification_commands": [
      "cat /opt/ai-q/docker/compose/logging-stack.yml",
      "docker-compose -f /opt/ai-q/docker/compose/logging-stack.yml config"
    ],
    "expected_outputs": {
      "config_valid": "Docker Compose configuration is valid",
      "services": "All 4 logging services defined"
    }
  },
  "verification_and_testing": {
    "comprehensive_test": {
      "title": "Complete Logging Stack Verification",
      "description": "Run comprehensive tests to verify all logging components",
      "test_commands": [
        "docker-compose -f /opt/ai-q/docker/compose/logging-stack.yml up -d",
        "sleep 60",
        "curl -s http://localhost:9200/_cluster/health",
        "curl -s http://localhost:5601/api/status",
        "curl -s http://localhost:9600/_node/stats",
        "docker logs ai-q-filebeat"
      ],
      "expected_results": {
        "elasticsearch": "Returns cluster health status",
        "kibana": "Returns status information",
        "logstash": "Returns node statistics",
        "filebeat": "Shows log collection activity"
      }
    }
  },
  "troubleshooting": {
    "common_issues": [
      {
        "issue": "Elasticsearch fails to start",
        "symptoms": "Elasticsearch container exits with error",
        "solution": "Check memory allocation and permissions",
        "commands": [
          "docker logs ai-q-elasticsearch",
          "free -h",
          "ls -la /opt/ai-q/logging/elasticsearch/"
        ]
      },
      {
        "issue": "Logstash cannot connect to Elasticsearch",
        "symptoms": "Logstash shows connection errors",
        "solution": "Check network connectivity and Elasticsearch status",
        "commands": [
          "docker network inspect ai-q-monitoring",
          "curl -s http://localhost:9200/_cluster/health",
          "docker logs ai-q-logstash"
        ]
      },
      {
        "issue": "Filebeat not collecting logs",
        "symptoms": "No logs appearing in Elasticsearch",
        "solution": "Check Filebeat configuration and Docker socket access",
        "commands": [
          "docker logs ai-q-filebeat",
          "ls -la /var/run/docker.sock",
          "cat /opt/ai-q/logging/filebeat/config/filebeat.yml"
        ]
      }
    ]
  },
  "next_steps": {
    "next_sub_recipe": "01-04-load-balancing",
    "prerequisites_completed": [
      "Elasticsearch installed and running",
      "Logstash configured with pipeline",
      "Kibana installed and accessible",
      "Filebeat collecting Docker logs",
      "All services communicating properly",
      "Logs flowing through the pipeline"
    ],
    "readiness_check": "All verification commands pass successfully"
  }
} 